{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBWwQSXuhaF3",
    "outputId": "8dc4057e-6f0a-4949-bf4c-feb680b4c93b",
    "ExecuteTime": {
     "end_time": "2024-12-10T14:58:21.207478Z",
     "start_time": "2024-12-10T14:58:21.204102Z"
    }
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:58:21.222011Z",
     "start_time": "2024-12-10T14:58:21.215491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "def resize_image_and_bboxes(image, bboxes, target_size):\n",
    "    \"\"\"\n",
    "    修改图像的大小并调整对应的边界框。\n",
    "\n",
    "    :param image: 原始图像 (PIL Image)\n",
    "    :param bboxes: 原始边界框 (Numpy 或 Tensor，形状为 [N, 4])，格式 [x_min, y_min, x_max, y_max]\n",
    "    :param target_size: 目标大小 (宽度, 高度)\n",
    "    :return: 调整大小后的图像和调整后的边界框\n",
    "    \"\"\"\n",
    "    # 获取原始图像的宽度和高度\n",
    "    orig_width, orig_height = image.size\n",
    "\n",
    "    # 调整图像的大小\n",
    "    image_resized = image.resize(target_size, Image.BILINEAR)\n",
    "\n",
    "    # 计算缩放比例\n",
    "    target_width, target_height = target_size\n",
    "    scale_x = target_width / orig_width\n",
    "    scale_y = target_height / orig_height\n",
    "\n",
    "    # 调整边界框\n",
    "    bboxes_resized = bboxes.clone()\n",
    "    bboxes_resized[:, [0, 2]] *= scale_x  # x_min 和 x_max 按照水平比例缩放\n",
    "    bboxes_resized[:, [1, 3]] *= scale_y  # y_min 和 y_max 按照垂直比例缩放\n",
    "\n",
    "    return image_resized, bboxes_resized\n",
    "\n",
    "\n",
    "def filter_invalid_boxes(boxes):\n",
    "        \"\"\"\n",
    "        过滤无效的边界框（宽度或高度为零的框）\n",
    "        \"\"\"\n",
    "        # 计算宽度和高度\n",
    "        width = boxes[:, 2] - boxes[:, 0]\n",
    "        height = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "        # 保留有效的框，宽度和高度大于0\n",
    "        valid_boxes = boxes[(width > 0) & (height > 0) & (boxes[:, 1] > 0) & (boxes[:, 0] > 0)]\n",
    "\n",
    "        # 如果没有有效框，返回一个空的框\n",
    "        if len(valid_boxes) == 0:\n",
    "            return np.empty((0, 4), dtype=np.float32)  # 空框\n",
    "\n",
    "        return valid_boxes\n",
    "\n",
    "\n",
    "def RandomHorizontalFlip(prob, images, targets):\n",
    "    for image, target in zip(images, targets):\n",
    "        if random.random() < prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-1)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "    return images, targets"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, json_file, transform=None):\n",
    "        \"\"\"\n",
    "        :param img_dir: 存放图像的文件夹路径\n",
    "        :param json_file: 包含标注信息的 JSON 文件路径\n",
    "        :param transform: 需要应用于图像的变换（可选）\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # 读取标注文件\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "\n",
    "        # 图像的ID列表\n",
    "        self.img_ids = [anno[\"id\"] for anno in self.annotations]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anno = self.annotations[idx]\n",
    "        img_path = os.path.join(self.img_dir, anno[\"id\"])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # 处理目标信息\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        if anno[\"region\"]:\n",
    "            boxes = np.array(anno[\"region\"], dtype=np.float32)\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)  # 转换为tensor\n",
    "            labels = torch.tensor([1], dtype=torch.int64)  # 篡改类为类1\n",
    "\n",
    "        else:\n",
    "            boxes = torch.empty((0, 4), dtype=torch.float32)  # 空的2D tensor，形状为 [0, 4]\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)  # 转换为tensor\n",
    "            labels = torch.tensor([0], dtype=torch.int64) # 未篡改类\n",
    "\n",
    "        # 检查框的有效性\n",
    "        if len(boxes) > 0:\n",
    "            # 过滤无效的框（宽度和高度为零的框）\n",
    "            boxes = self.filter_invalid_boxes(boxes)\n",
    "\n",
    "        # 如果没有有效框，跳过该样本\n",
    "        if len(boxes) == 0:\n",
    "            boxes = torch.empty((0, 4), dtype=torch.float32)  # 空的2D tensor，形状为 [0, 4]\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)  # 转换为tensor\n",
    "            labels = torch.tensor([0], dtype=torch.int64) # 未篡改类\n",
    "            # return self.__getitem__((idx + 1) % len(self))  # 递归调用获取下一个有效样本\n",
    "        \n",
    "        # img, boxes = resize_image_and_bboxes(img, boxes, (512, 512))\n",
    "        \n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def filter_invalid_boxes(self, boxes):\n",
    "        \"\"\"\n",
    "        过滤无效的边界框（宽度或高度为零的框）\n",
    "        \"\"\"\n",
    "        # 计算宽度和高度\n",
    "        width = boxes[:, 2] - boxes[:, 0]\n",
    "        height = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "        # 保留有效的框，宽度和高度大于0\n",
    "        valid_boxes = boxes[(width > 0) & (height > 0) & (boxes[:, 1] > 0) & (boxes[:, 0] > 0)]\n",
    "\n",
    "        # 如果没有有效框，返回一个空的框\n",
    "        if len(valid_boxes) == 0:\n",
    "            return np.empty((0, 4), dtype=np.float32)  # 空框\n",
    "\n",
    "        return valid_boxes\n",
    "\n",
    "# 定义测试集的数据集类（与训练集相同，但不需要标签）\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        :param img_dir: 存放测试图像的文件夹路径\n",
    "        :param transform: 需要应用于图像的变换（可选）\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_ids = os.listdir(img_dir)  # 获取所有图片的文件名\n",
    "        self.original_sizes = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_id)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        self.original_sizes[img_id] = img.size\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, img_id  # 只返回图像和图片ID\n",
    "\n",
    "# 加载模型\n",
    "def load_model(model, filepath):\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()  # 切换到评估模式\n",
    "    print(f\"模型已从 {filepath} 加载\")\n",
    "    return model\n",
    "\n",
    "# 图像转换操作\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "])\n",
    "\n",
    "# 定义训练数据集和数据加载器\n",
    "train_dataset = CustomDataset(img_dir=\"data/image/train\", json_file=\"data/label_train.json\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# 定义测试数据集和数据加载器\n",
    "test_dataset = TestDataset(img_dir=\"data/image/val\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 加载预训练的Faster R-CNN模型\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)\n",
    "\n",
    "# model = fasterrcnn_mobilenet_v3_large_fpn(pretained=True)\n",
    "# model = torchvision.models.detection.retinanet_resnet50_fpn_v2(pretrained=True)\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# model.roi_heads.box_predictor = torchvision.models.detection.retinanet.FastRCNNPredictor(in_features, 2)\n",
    "\n",
    "# 加载训练好的模型\n",
    "# load_model(model, 'model/model0.pth')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 定义优化器\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=1e-3, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# 定义学习率调度器\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n"
   ],
   "metadata": {
    "id": "MIJlXLrUitNE",
    "ExecuteTime": {
     "end_time": "2024-12-10T14:58:21.943319Z",
     "start_time": "2024-12-10T14:58:21.223014Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# 保存模型函数\n",
    "def save_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"模型已保存到 {filepath}\")\n",
    "\n",
    "# 定义训练函数\n",
    "def train_model(model, dataloader, optimizer, lr_scheduler, num_epochs, save_path=\"model\"):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for images, targets in tqdm(dataloader):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # images, targets = RandomHorizontalFlip(0.95, images, targets)\n",
    "\n",
    "            # 前向传播\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            # 计算总损失\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            epoch_loss += losses.item()\n",
    "\n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss / len(dataloader)}\")\n",
    "        # 每个 epoch 后保存模型\n",
    "        current_path = os.path.join(save_path, \"model\" + str(epoch + 1) + \".pth\")\n",
    "        save_model(model, current_path)\n",
    "\n",
    "    # # 每个 epoch 后保存模型\n",
    "    # save_model(model, save_path)\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_loader, optimizer, lr_scheduler, num_epochs=10)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_Fp-bTClCpi",
    "outputId": "3cefe7bf-3cd5-4d0b-8911-a1b2479ae61e",
    "ExecuteTime": {
     "end_time": "2024-12-11T00:48:53.062839Z",
     "start_time": "2024-12-10T14:58:21.944360Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/13000 [00:00<48:46,  4.44it/s]C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1596\\2941823363.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes = torch.tensor(boxes, dtype=torch.float32)  # 转换为tensor\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_1596\\2941823363.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes = torch.tensor(boxes, dtype=torch.float32)  # 转换为tensor\n",
      "100%|██████████| 13000/13000 [56:07<00:00,  3.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.062497120844769806\n",
      "模型已保存到 model\\model1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13000/13000 [53:28<00:00,  4.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.041640069077911415\n",
      "模型已保存到 model\\model2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13000/13000 [1:03:40<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.033013863221624466\n",
      "模型已保存到 model\\model3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13000/13000 [59:22<00:00,  3.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.02856314250791939\n",
      "模型已保存到 model\\model4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13000/13000 [58:49<00:00,  3.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.026133075703909946\n",
      "模型已保存到 model\\model5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13000/13000 [59:49<00:00,  3.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.025181561846887374\n",
      "模型已保存到 model\\model6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13000/13000 [59:49<00:00,  3.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.024489155830559866\n",
      "模型已保存到 model\\model7.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13000/13000 [59:53<00:00,  3.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.024290251061632927\n",
      "模型已保存到 model\\model8.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13000/13000 [1:00:00<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.024099694278925898\n",
      "模型已保存到 model\\model9.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13000/13000 [59:25<00:00,  3.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.024013405487110992\n",
      "模型已保存到 model\\model10.pth\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T00:48:53.911490Z",
     "start_time": "2024-12-11T00:48:53.091838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义测试集的数据集类（与训练集相同，但不需要标签）\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        :param img_dir: 存放测试图像的文件夹路径\n",
    "        :param transform: 需要应用于图像的变换（可选）\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_ids = os.listdir(img_dir)  # 获取所有图片的文件名\n",
    "        self.original_sizes = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_id)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        self.original_sizes[img_id] = img.size\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, img_id  # 只返回图像和图片ID\n",
    "\n",
    "# 加载模型\n",
    "def load_model(model, filepath):\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()  # 切换到评估模式\n",
    "    print(f\"模型已从 {filepath} 加载\")\n",
    "    return model\n",
    "\n",
    "# 进行推理并生成结果\n",
    "def generate_predictions(model, dataloader, output_json_path):\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        model.eval()  # 切换到评估模式\n",
    "        for images, img_ids in tqdm(dataloader):\n",
    "            images = [image.to(device) for image in images]\n",
    "\n",
    "            # 模型推理\n",
    "            predictions = model(images)\n",
    "\n",
    "\n",
    "            for i, img_id in enumerate(img_ids):\n",
    "                prediction = predictions[i]\n",
    "\n",
    "                # 获取预测框和标签\n",
    "                boxes = prediction['boxes'].cpu().numpy()\n",
    "                labels = prediction['labels'].cpu().numpy()\n",
    "                scores = prediction['scores'].cpu().numpy()\n",
    "\n",
    "                # 只保留标签为1的框，即篡改的区域，阈值可根据需要调整\n",
    "                mask = labels == 1\n",
    "                boxes = boxes[mask]\n",
    "                scores = scores[mask]\n",
    "                region = []\n",
    "\n",
    "                if len(boxes) > 0:\n",
    "                    # original_width, original_height = dataloader.dataset.original_sizes[img_id]  # 获取原始图像的大小\n",
    "                    # scale_x = original_width / 800  # 计算宽度的缩放比例\n",
    "                    # scale_y = original_height / 800  # 计算高度的缩放比例\n",
    "                    #\n",
    "                    # 将预测框的坐标缩放回原始图像的大小\n",
    "                    # boxes = boxes * np.array([scale_x, scale_y, scale_x, scale_y])  # 还原bbox大小\n",
    "\n",
    "                    best_idx = np.argmax(scores)\n",
    "                    best_box = boxes[best_idx]\n",
    "                    best_score = scores[best_idx]\n",
    "\n",
    "                    # 将最高置信度的框添加到结果中\n",
    "                    region = [best_box.tolist()]  # 转换为原生类型\n",
    "\n",
    "                # 如果有预测框，保存预测框\n",
    "                # region = boxes.tolist() if len(boxes) > 0 else []\n",
    "\n",
    "                # 将结果添加到列表中\n",
    "                results.append({\"id\": img_id, \"region\": region})\n",
    "\n",
    "    # 将结果保存为JSON文件\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"预测结果已保存到 {output_json_path}\")\n",
    "\n",
    "# 图像转换操作\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "    # transforms.Resize((800, 800)),  # 统一图像大小\n",
    "])\n",
    "\n",
    "# 定义测试数据集和数据加载器\n",
    "test_dataset = TestDataset(img_dir=\"data/image/val\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "\n",
    "# 修改模型的分类头部分，将类别数改为2（篡改和未篡改）\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)\n",
    "# 修改模型的分类头部分，将类别数改为2（篡改和未篡改）\n",
    "load_model(model, 'model/model7.pth')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "generate_predictions(model, test_loader, \"output/label_test.json\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\". \n\tUnexpected key(s) in state_dict: \"backbone.fpn.inner_blocks.0.1.weight\", \"backbone.fpn.inner_blocks.0.1.bias\", \"backbone.fpn.inner_blocks.0.1.running_mean\", \"backbone.fpn.inner_blocks.0.1.running_var\", \"backbone.fpn.inner_blocks.0.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.1.1.weight\", \"backbone.fpn.inner_blocks.1.1.bias\", \"backbone.fpn.inner_blocks.1.1.running_mean\", \"backbone.fpn.inner_blocks.1.1.running_var\", \"backbone.fpn.inner_blocks.1.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.2.1.weight\", \"backbone.fpn.inner_blocks.2.1.bias\", \"backbone.fpn.inner_blocks.2.1.running_mean\", \"backbone.fpn.inner_blocks.2.1.running_var\", \"backbone.fpn.inner_blocks.2.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.3.1.weight\", \"backbone.fpn.inner_blocks.3.1.bias\", \"backbone.fpn.inner_blocks.3.1.running_mean\", \"backbone.fpn.inner_blocks.3.1.running_var\", \"backbone.fpn.inner_blocks.3.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.0.1.weight\", \"backbone.fpn.layer_blocks.0.1.bias\", \"backbone.fpn.layer_blocks.0.1.running_mean\", \"backbone.fpn.layer_blocks.0.1.running_var\", \"backbone.fpn.layer_blocks.0.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.1.1.weight\", \"backbone.fpn.layer_blocks.1.1.bias\", \"backbone.fpn.layer_blocks.1.1.running_mean\", \"backbone.fpn.layer_blocks.1.1.running_var\", \"backbone.fpn.layer_blocks.1.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.2.1.weight\", \"backbone.fpn.layer_blocks.2.1.bias\", \"backbone.fpn.layer_blocks.2.1.running_mean\", \"backbone.fpn.layer_blocks.2.1.running_var\", \"backbone.fpn.layer_blocks.2.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.3.1.weight\", \"backbone.fpn.layer_blocks.3.1.bias\", \"backbone.fpn.layer_blocks.3.1.running_mean\", \"backbone.fpn.layer_blocks.3.1.running_var\", \"backbone.fpn.layer_blocks.3.1.num_batches_tracked\", \"rpn.head.conv.1.0.weight\", \"rpn.head.conv.1.0.bias\", \"roi_heads.box_head.0.0.weight\", \"roi_heads.box_head.0.1.weight\", \"roi_heads.box_head.0.1.bias\", \"roi_heads.box_head.0.1.running_mean\", \"roi_heads.box_head.0.1.running_var\", \"roi_heads.box_head.0.1.num_batches_tracked\", \"roi_heads.box_head.1.0.weight\", \"roi_heads.box_head.1.1.weight\", \"roi_heads.box_head.1.1.bias\", \"roi_heads.box_head.1.1.running_mean\", \"roi_heads.box_head.1.1.running_var\", \"roi_heads.box_head.1.1.num_batches_tracked\", \"roi_heads.box_head.2.0.weight\", \"roi_heads.box_head.2.1.weight\", \"roi_heads.box_head.2.1.bias\", \"roi_heads.box_head.2.1.running_mean\", \"roi_heads.box_head.2.1.running_var\", \"roi_heads.box_head.2.1.num_batches_tracked\", \"roi_heads.box_head.3.0.weight\", \"roi_heads.box_head.3.1.weight\", \"roi_heads.box_head.3.1.bias\", \"roi_heads.box_head.3.1.running_mean\", \"roi_heads.box_head.3.1.running_var\", \"roi_heads.box_head.3.1.num_batches_tracked\", \"roi_heads.box_head.5.weight\", \"roi_heads.box_head.5.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 82\u001B[0m\n\u001B[0;32m     80\u001B[0m model\u001B[38;5;241m.\u001B[39mroi_heads\u001B[38;5;241m.\u001B[39mbox_predictor \u001B[38;5;241m=\u001B[39m torchvision\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mdetection\u001B[38;5;241m.\u001B[39mfaster_rcnn\u001B[38;5;241m.\u001B[39mFastRCNNPredictor(in_features, num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     81\u001B[0m \u001B[38;5;66;03m# 修改模型的分类头部分，将类别数改为2（篡改和未篡改）\u001B[39;00m\n\u001B[1;32m---> 82\u001B[0m load_model(model, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel/model.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     83\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     84\u001B[0m model\u001B[38;5;241m.\u001B[39mto(device)\n",
      "Cell \u001B[1;32mIn[9], line 16\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(model, filepath)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_model\u001B[39m(model, filepath):\n\u001B[1;32m---> 16\u001B[0m     model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(filepath))\n\u001B[0;32m     17\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()  \u001B[38;5;66;03m# 切换到评估模式\u001B[39;00m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m模型已从 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m 加载\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[1;34m(self, state_dict, strict)\u001B[0m\n\u001B[0;32m   2036\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[0;32m   2037\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2038\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[0;32m   2040\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2041\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   2042\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[0;32m   2043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\". \n\tUnexpected key(s) in state_dict: \"backbone.fpn.inner_blocks.0.1.weight\", \"backbone.fpn.inner_blocks.0.1.bias\", \"backbone.fpn.inner_blocks.0.1.running_mean\", \"backbone.fpn.inner_blocks.0.1.running_var\", \"backbone.fpn.inner_blocks.0.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.1.1.weight\", \"backbone.fpn.inner_blocks.1.1.bias\", \"backbone.fpn.inner_blocks.1.1.running_mean\", \"backbone.fpn.inner_blocks.1.1.running_var\", \"backbone.fpn.inner_blocks.1.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.2.1.weight\", \"backbone.fpn.inner_blocks.2.1.bias\", \"backbone.fpn.inner_blocks.2.1.running_mean\", \"backbone.fpn.inner_blocks.2.1.running_var\", \"backbone.fpn.inner_blocks.2.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.3.1.weight\", \"backbone.fpn.inner_blocks.3.1.bias\", \"backbone.fpn.inner_blocks.3.1.running_mean\", \"backbone.fpn.inner_blocks.3.1.running_var\", \"backbone.fpn.inner_blocks.3.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.0.1.weight\", \"backbone.fpn.layer_blocks.0.1.bias\", \"backbone.fpn.layer_blocks.0.1.running_mean\", \"backbone.fpn.layer_blocks.0.1.running_var\", \"backbone.fpn.layer_blocks.0.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.1.1.weight\", \"backbone.fpn.layer_blocks.1.1.bias\", \"backbone.fpn.layer_blocks.1.1.running_mean\", \"backbone.fpn.layer_blocks.1.1.running_var\", \"backbone.fpn.layer_blocks.1.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.2.1.weight\", \"backbone.fpn.layer_blocks.2.1.bias\", \"backbone.fpn.layer_blocks.2.1.running_mean\", \"backbone.fpn.layer_blocks.2.1.running_var\", \"backbone.fpn.layer_blocks.2.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.3.1.weight\", \"backbone.fpn.layer_blocks.3.1.bias\", \"backbone.fpn.layer_blocks.3.1.running_mean\", \"backbone.fpn.layer_blocks.3.1.running_var\", \"backbone.fpn.layer_blocks.3.1.num_batches_tracked\", \"rpn.head.conv.1.0.weight\", \"rpn.head.conv.1.0.bias\", \"roi_heads.box_head.0.0.weight\", \"roi_heads.box_head.0.1.weight\", \"roi_heads.box_head.0.1.bias\", \"roi_heads.box_head.0.1.running_mean\", \"roi_heads.box_head.0.1.running_var\", \"roi_heads.box_head.0.1.num_batches_tracked\", \"roi_heads.box_head.1.0.weight\", \"roi_heads.box_head.1.1.weight\", \"roi_heads.box_head.1.1.bias\", \"roi_heads.box_head.1.1.running_mean\", \"roi_heads.box_head.1.1.running_var\", \"roi_heads.box_head.1.1.num_batches_tracked\", \"roi_heads.box_head.2.0.weight\", \"roi_heads.box_head.2.1.weight\", \"roi_heads.box_head.2.1.bias\", \"roi_heads.box_head.2.1.running_mean\", \"roi_heads.box_head.2.1.running_var\", \"roi_heads.box_head.2.1.num_batches_tracked\", \"roi_heads.box_head.3.0.weight\", \"roi_heads.box_head.3.1.weight\", \"roi_heads.box_head.3.1.bias\", \"roi_heads.box_head.3.1.running_mean\", \"roi_heads.box_head.3.1.running_var\", \"roi_heads.box_head.3.1.num_batches_tracked\", \"roi_heads.box_head.5.weight\", \"roi_heads.box_head.5.bias\". "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T00:48:53.912999Z",
     "start_time": "2024-12-11T00:48:53.912999Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
